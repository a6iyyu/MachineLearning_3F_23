{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a6iyyu/MachineLearning_3F_23/blob/main/JS6/P6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Praktikum 6**\n",
        "\n",
        "Lakukan percobaan penggunaan ANNOY, FAISS, dan HNSWLIB pada dataset sekunder berukuran besar (Micro Spotify) pada link berikut: https://www.kaggle.com/datasets/bwandowando/spotify-songs-with-attributes-and-lyrics/data. Download data dan load CSV filenya (pilih dataset yg pertama dari dua dataset). pilih hanya fitur numerik saja, dan lakukan normalisasi menggunakan StandardScaler. Lakukan pencarian track terdekat dan bandingkan hasilnya."
      ],
      "metadata": {
        "id": "2lyHRA_3Vztw"
      },
      "id": "2lyHRA_3Vztw"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn annoy hnswlib faiss-cpu opendatasets"
      ],
      "metadata": {
        "id": "uhsOZPwGVvUj",
        "outputId": "9bad50b8-467d-46fc-f7bc-1483f7486314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uhsOZPwGVvUj",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hnswlib\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: annoy, hnswlib\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.3-cp312-cp312-linux_x86_64.whl size=551811 sha256=d77a961f459243f5bb0d482f13a84329101682dec889027d0f7c6f5907c7a961\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/b9/53/a3b2d1fe1743abadddec6aa541294b24fdbc39d7800bc57311\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp312-cp312-linux_x86_64.whl size=2528144 sha256=4bf1e7707782067e7cd50482b36503a9a9120201cc5a835d4dba3fac696051d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/39/b3/cbd7f9cbb76501d2d5fbc84956e70d0b94e788aac87bda465e\n",
            "Successfully built annoy hnswlib\n",
            "Installing collected packages: annoy, hnswlib, faiss-cpu, opendatasets\n",
            "Successfully installed annoy-1.17.3 faiss-cpu-1.12.0 hnswlib-0.8.0 opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import faiss\n",
        "from annoy import AnnoyIndex\n",
        "import hnswlib\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import opendatasets as od\n",
        "import os"
      ],
      "metadata": {
        "id": "G7sX9effVpki"
      },
      "id": "G7sX9effVpki",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30260732",
      "metadata": {
        "id": "30260732",
        "outputId": "7e1c4bbe-61fa-47ab-abe7-0f0ecad27f63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File dataset tidak ditemukan. Mengunduh dari Kaggle...\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username:"
          ]
        }
      ],
      "source": [
        "dataset_url = 'https://www.kaggle.com/datasets/bwandowando/spotify-songs-with-attributes-and-lyrics'\n",
        "dataset_dir = 'spotify-songs-with-attributes-and-lyrics'\n",
        "\n",
        "# Path ke file CSV yang akan digunakan (dataset pertama)\n",
        "csv_path = os.path.join(dataset_dir, 'songs_with_attributes_and_lyrics.csv')\n",
        "\n",
        "# Cek apakah dataset sudah ada, jika tidak, unduh\n",
        "if not os.path.exists(csv_path):\n",
        "    print(\"File dataset tidak ditemukan. Mengunduh dari Kaggle...\")\n",
        "    od.download(dataset_url)\n",
        "else:\n",
        "    print(\"Dataset sudah tersedia di direktori.\")\n",
        "\n",
        "print(f\"Membaca file dari: {csv_path}\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"Dataset berhasil dimuat.\")\n",
        "    print(f\"Jumlah data: {len(df)} lagu\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File tidak ditemukan di path: {csv_path}\")\n",
        "    print(\"Pastikan proses download berhasil dan path file sudah benar.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"Dataset berhasil dimuat.\")\n",
        "    print(f\"Jumlah data: {len(df)} lagu\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File tidak ditemukan di path: {csv_path}\")\n",
        "    print(\"Pastikan proses download berhasil dan path file sudah benar.\")"
      ],
      "metadata": {
        "id": "n6x3Y4OxXnTU"
      },
      "id": "n6x3Y4OxXnTU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pilih hanya fitur numerik yang relevan untuk perbandingan kemiripan lagu\n",
        "features = ['danceability', 'energy', 'loudness', 'speechiness',\n",
        "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
        "X = df[features].values.astype('float32') # FAISS bekerja lebih baik dengan float32"
      ],
      "metadata": {
        "id": "NuwXE7wwb7-9"
      },
      "id": "NuwXE7wwb7-9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 2. Standarisasi Fitur\n",
        "# -------------------------------\n",
        "# Scaling penting agar fitur dengan rentang nilai besar (misal: tempo)\n",
        "# tidak mendominasi fitur dengan rentang kecil (misal: danceability).\n",
        "print(\"\\nMelakukan standarisasi fitur...\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Standarisasi selesai.\")\n",
        "\n",
        "# Jumlah nearest neighbors yang akan dicari\n",
        "k = 10\n",
        "\n",
        "print(\"\\nMemulai perbandingan metode Nearest Neighbor Search...\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "NROck2WQb-Ym"
      },
      "id": "NROck2WQb-Ym",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Metode 1: Exact Nearest Neighbor (Brute-force) sebagai Baseline\n",
        "# -------------------------------\n",
        "start_time = time.time()\n",
        "nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
        "nn.fit(X_scaled)\n",
        "# Mencari tetangga terdekat untuk semua titik data\n",
        "dist_exact, idx_exact = nn.kneighbors(X_scaled)\n",
        "time_exact = time.time() - start_time\n",
        "print(f\"Exact NN (Brute-force) selesai dalam: {time_exact:.4f} detik\")"
      ],
      "metadata": {
        "id": "9A6X8iUEcAF6"
      },
      "id": "9A6X8iUEcAF6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Metode 2: Annoy (Approximate Nearest Neighbors Oh Yeah)\n",
        "# -------------------------------\n",
        "start_time = time.time()\n",
        "num_features = X_scaled.shape[1]\n",
        "index_annoy = AnnoyIndex(num_features, 'euclidean')\n",
        "for i, vec in enumerate(X_scaled):\n",
        "    index_annoy.add_item(i, vec)\n",
        "\n",
        "# Membangun index. Angka 10 adalah jumlah pohon (n_trees).\n",
        "# Semakin banyak pohon, semakin akurat hasilnya, tapi semakin lama proses build-nya.\n",
        "index_annoy.build(10)\n",
        "\n",
        "# Mencari tetangga\n",
        "idx_annoy = []\n",
        "for vec in X_scaled:\n",
        "    idx_annoy.append(index_annoy.get_nns_by_vector(vec, k))\n",
        "time_annoy = time.time() - start_time\n",
        "print(f\"Annoy selesai dalam:                   {time_annoy:.4f} detik\")"
      ],
      "metadata": {
        "id": "uLOG1EPScCW-"
      },
      "id": "uLOG1EPScCW-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Metode 3: HNSW (Hierarchical Navigable Small World)\n",
        "# -------------------------------\n",
        "start_time = time.time()\n",
        "dim = X_scaled.shape[1]\n",
        "num_elements = X_scaled.shape[0]\n",
        "\n",
        "# Deklarasi index\n",
        "p_hnsw = hnswlib.Index(space='l2', dim=dim) # l2 space = euclidean distance\n",
        "\n",
        "# Inisialisasi index. Parameter M dan ef_construction mempengaruhi akurasi vs kecepatan build.\n",
        "p_hnsw.init_index(max_elements=num_elements, ef_construction=200, M=16)\n",
        "\n",
        "# Menambahkan data ke index\n",
        "p_hnsw.add_items(X_scaled)\n",
        "\n",
        "# Mengatur parameter pencarian (ef). Semakin tinggi, semakin akurat tapi lebih lambat.\n",
        "p_hnsw.set_ef(200)\n",
        "\n",
        "# Melakukan pencarian\n",
        "idx_hnsw, dist_hnsw = p_hnsw.knn_query(X_scaled, k=k)\n",
        "time_hnsw = time.time() - start_time\n",
        "print(f\"HNSW selesai dalam:                     {time_hnsw:.4f} detik\")"
      ],
      "metadata": {
        "id": "2EDnHi0zcFA1"
      },
      "id": "2EDnHi0zcFA1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Metode 4: FAISS (Facebook AI Similarity Search) - IVF\n",
        "# -------------------------------\n",
        "start_time = time.time()\n",
        "dim = X_scaled.shape[1]\n",
        "nlist = 100  # Jumlah sel/partisi. Aturan umum: akar kuadrat dari jumlah data.\n",
        "\n",
        "# Quantizer untuk mempartisi data\n",
        "quantizer = faiss.IndexFlatL2(dim)\n",
        "# Index IVF (Inverted File) yang menggunakan quantizer\n",
        "index_faiss = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_L2)\n",
        "\n",
        "# Training index untuk mempelajari distribusi data dan membuat partisi\n",
        "index_faiss.train(X_scaled)\n",
        "# Menambahkan data ke index\n",
        "index_faiss.add(X_scaled)\n",
        "\n",
        "# nprobe: Berapa banyak partisi terdekat yang akan dicari.\n",
        "# Semakin tinggi, semakin akurat tapi semakin lambat.\n",
        "index_faiss.nprobe = 10\n",
        "\n",
        "# Melakukan pencarian\n",
        "dist_faiss, idx_faiss = index_faiss.search(X_scaled, k)\n",
        "time_faiss = time.time() - start_time\n",
        "print(f\"FAISS IVF selesai dalam:                {time_faiss:.4f} detik\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "9lmU1OOIcHbp"
      },
      "id": "9lmU1OOIcHbp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# 5. Perbandingan Hasil\n",
        "# -------------------------------\n",
        "# Kita akan membandingkan hasil pencarian untuk lagu pertama dalam dataset.\n",
        "# Lagu pertama dianggap sebagai dirinya sendiri, jadi kita lihat 5 tetangga setelahnya.\n",
        "query_song_index = 0\n",
        "num_neighbors_to_show = 6 # Termasuk dirinya sendiri\n",
        "\n",
        "print(f\"\\nTop-{num_neighbors_to_show-1} neighbors untuk lagu pertama (index {query_song_index}):\")\n",
        "print(f\"Lagu Asli: '{df.iloc[query_song_index]}' oleh {df.iloc[query_song_index]['track_artist']}\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "1BGtNfFQb4MU"
      },
      "id": "1BGtNfFQb4MU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk menghitung recall@k\n",
        "def calculate_recall(ground_truth, approx_results):\n",
        "    correct_predictions = 0\n",
        "    total_queries = len(ground_truth)\n",
        "    for i in range(total_queries):\n",
        "        # Mengabaikan item pertama karena itu adalah item itu sendiri\n",
        "        true_neighbors = set(ground_truth[i][1:])\n",
        "        approx_neighbors = set(approx_results[i][1:])\n",
        "        correct_predictions += len(true_neighbors.intersection(approx_neighbors))\n",
        "\n",
        "    # Total tetangga yang seharusnya ditemukan (k-1 per query)\n",
        "    total_neighbors = total_queries * (k - 1)\n",
        "    return correct_predictions / total_neighbors if total_neighbors > 0 else 0"
      ],
      "metadata": {
        "id": "2BvQhBgBcMRe"
      },
      "id": "2BvQhBgBcMRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan hasil dan hitung akurasi (recall)\n",
        "recall_annoy = calculate_recall(idx_exact, idx_annoy)\n",
        "recall_hnsw = calculate_recall(idx_exact, idx_hnsw.tolist())\n",
        "recall_faiss = calculate_recall(idx_exact, idx_faiss.tolist())\n",
        "\n",
        "print(f\"Baseline (Exact): {idx_exact[query_song_index][:num_neighbors_to_show]}\")\n",
        "print(f\"Annoy:            {np.array(idx_annoy)[query_song_index][:num_neighbors_to_show]} (Recall: {recall_annoy:.4f})\")\n",
        "print(f\"HNSW:             {idx_hnsw[query_song_index][:num_neighbors_to_show]} (Recall: {recall_hnsw:.4f})\")\n",
        "print(f\"FAISS:            {idx_faiss[query_song_index][:num_neighbors_to_show]} (Recall: {recall_faiss:.4f})\")"
      ],
      "metadata": {
        "id": "A1yP0rf5cJ4M"
      },
      "id": "A1yP0rf5cJ4M",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}